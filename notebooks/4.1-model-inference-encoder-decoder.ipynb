{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:23.308019Z",
     "start_time": "2023-10-26T12:24:21.486381800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PATH_OUT = \"../models/gru-detoxification/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparation for Inference "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f9c922b6c97463d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           reference  \\\n0  If Alkar is flooding her with psychic waste, t...   \n1                          Now you're getting nasty.   \n2           Well, we could spare your life, for one.   \n3          Ah! Monkey, you've got to snap out of it.   \n4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff  \\\n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  avg_word_ref  avg_word_trans  \n0  0.981983  0.014195            15              16  \n1  0.999039  0.065473             4               3  \n2  0.985068  0.213313             8               6  \n3  0.994215  0.053362             9               6  \n4  0.999348  0.009402             7               6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n      <th>avg_word_ref</th>\n      <th>avg_word_trans</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.981983</td>\n      <td>0.014195</td>\n      <td>15</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.999039</td>\n      <td>0.065473</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.985068</td>\n      <td>0.213313</td>\n      <td>8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.994215</td>\n      <td>0.053362</td>\n      <td>9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.999348</td>\n      <td>0.009402</td>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"../data/inheritim/\"\n",
    "df = pd.read_csv(PATH + 'filtered.csv', index_col=0)#.sample(10000)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:24.392328100Z",
     "start_time": "2023-10-26T12:24:23.310011600Z"
    }
   },
   "id": "10ddb81e9b6d1d01"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           reference  \\\n0  <start> if alkar is flooding her with psychic ...   \n1           <start> now you re getting nasty . <end>   \n2  <start> well , we could spare your life , for ...   \n3  <start> ah ! monkey , you ve got to snap out o...   \n4    <start> i ve got orders to put her down . <end>   \n5  <start> i m not gonna have a child . . . . . ....   \n6  <start> they re all laughing at us , so we ll ...   \n7  <start> maine was very short on black people b...   \n8  <start> briggs , what the hell s happening ? <...   \n9  <start> another one simply had no clue what to...   \n\n                                         translation  similarity  lenght_diff  \\\n0  <start> if alkar floods her with her mental wa...    0.785171     0.010309   \n1         <start> you re becoming disgusting . <end>    0.749687     0.071429   \n2      <start> well , we can spare your life . <end>    0.919051     0.268293   \n3       <start> monkey , you have to wake up . <end>    0.664333     0.309524   \n4          <start> i have orders to kill her . <end>    0.726639     0.181818   \n5  <start> i m not going to breed kids with a gen...    0.703185     0.206522   \n6  <start> they re laughing at us . we ll show yo...    0.618866     0.230769   \n7  <start> there wasn t much black in maine then ...    0.720482     0.187500   \n8  <start> briggs , what the hell is going on ? <...    0.920373     0.000000   \n9  <start> another simply didn t know what to do ...    0.877540     0.101695   \n\n    ref_tox   trn_tox  avg_word_ref  avg_word_trans  \n0  0.981983  0.014195            15              16  \n1  0.999039  0.065473             4               3  \n2  0.985068  0.213313             8               6  \n3  0.994215  0.053362             9               6  \n4  0.999348  0.009402             7               6  \n5  0.950956  0.035846            17              14  \n6  0.999492  0.000131            10               7  \n7  0.963680  0.148710             9               7  \n8  0.841071  0.159096             5               7  \n9  0.930472  0.055371            25              21  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n      <th>avg_word_ref</th>\n      <th>avg_word_trans</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;start&gt; if alkar is flooding her with psychic ...</td>\n      <td>&lt;start&gt; if alkar floods her with her mental wa...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.981983</td>\n      <td>0.014195</td>\n      <td>15</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;start&gt; now you re getting nasty . &lt;end&gt;</td>\n      <td>&lt;start&gt; you re becoming disgusting . &lt;end&gt;</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.999039</td>\n      <td>0.065473</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;start&gt; well , we could spare your life , for ...</td>\n      <td>&lt;start&gt; well , we can spare your life . &lt;end&gt;</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.985068</td>\n      <td>0.213313</td>\n      <td>8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;start&gt; ah ! monkey , you ve got to snap out o...</td>\n      <td>&lt;start&gt; monkey , you have to wake up . &lt;end&gt;</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.994215</td>\n      <td>0.053362</td>\n      <td>9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;start&gt; i ve got orders to put her down . &lt;end&gt;</td>\n      <td>&lt;start&gt; i have orders to kill her . &lt;end&gt;</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.999348</td>\n      <td>0.009402</td>\n      <td>7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>&lt;start&gt; i m not gonna have a child . . . . . ....</td>\n      <td>&lt;start&gt; i m not going to breed kids with a gen...</td>\n      <td>0.703185</td>\n      <td>0.206522</td>\n      <td>0.950956</td>\n      <td>0.035846</td>\n      <td>17</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>&lt;start&gt; they re all laughing at us , so we ll ...</td>\n      <td>&lt;start&gt; they re laughing at us . we ll show yo...</td>\n      <td>0.618866</td>\n      <td>0.230769</td>\n      <td>0.999492</td>\n      <td>0.000131</td>\n      <td>10</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>&lt;start&gt; maine was very short on black people b...</td>\n      <td>&lt;start&gt; there wasn t much black in maine then ...</td>\n      <td>0.720482</td>\n      <td>0.187500</td>\n      <td>0.963680</td>\n      <td>0.148710</td>\n      <td>9</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>&lt;start&gt; briggs , what the hell s happening ? &lt;...</td>\n      <td>&lt;start&gt; briggs , what the hell is going on ? &lt;...</td>\n      <td>0.920373</td>\n      <td>0.000000</td>\n      <td>0.841071</td>\n      <td>0.159096</td>\n      <td>5</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>&lt;start&gt; another one simply had no clue what to...</td>\n      <td>&lt;start&gt; another simply didn t know what to do ...</td>\n      <td>0.877540</td>\n      <td>0.101695</td>\n      <td>0.930472</td>\n      <td>0.055371</td>\n      <td>25</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unicode_to_ascii(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes latin chars with accent to their canonical decomposition\n",
    "\n",
    "    :param s: input sentence\n",
    "    :return: normalized sentence\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w) -> str:\n",
    "    \"\"\"\n",
    "    preprocess the sentence\n",
    "    :param w: input sentence\n",
    "    :return: preprocessed sentence\n",
    "    \"\"\"\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "df['reference'] = df['reference'].apply(lambda w: preprocess_sentence(w))\n",
    "df['translation'] = df['translation'].apply(lambda w: preprocess_sentence(w))\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:40.707787800Z",
     "start_time": "2023-10-26T12:24:24.393324900Z"
    }
   },
   "id": "935140e716f406c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create custom vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3311327917de9f2a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang: list):\n",
    "        \"\"\" lang are the list of phrases from each language \"\"\"\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\" create word2idx, idx2word and vocab \"\"\"\n",
    "        for phrase in self.lang:\n",
    "            # update with individual tokens\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        # sort the vocab\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        # add a padding token with index 0\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        # word to index mapping\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
    "        # index to word mapping\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:40.712842Z",
     "start_time": "2023-10-26T12:24:40.708784Z"
    }
   },
   "id": "948dfb44efc74956"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def max_length(tensor: list) -> int:\n",
    "    \"\"\"\n",
    "    calculate the max_length of input and output tensor\n",
    "    :param tensor: input tensor\n",
    "    :return: max length\n",
    "    \"\"\"\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "# index language using the class above\n",
    "inp_lang = LanguageIndex(df['reference'].values.tolist())\n",
    "targ_lang = LanguageIndex(df['translation'].values.tolist())\n",
    "\n",
    "# Vectorize the input and target languages\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in reference.split(' ')]  for reference in df['reference'].values.tolist()]\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in translation.split(' ')]  for translation in df['translation'].values.tolist()]\n",
    "\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:45.305120200Z",
     "start_time": "2023-10-26T12:24:40.712842Z"
    }
   },
   "id": "62b365d7587af9a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set architecture of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87585fc7bd3d0bc4"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, enc_units: int, batch_sz: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: torch.device) -> (torch.Tensor, torch.Tensor):\n",
    "        # x: batch_size, max_length \n",
    "\n",
    "        # x: batch_size, max_length, embedding_dim\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        self.hidden = self.initialize_hidden_state(device)\n",
    "\n",
    "        # output: max_length, batch_size, enc_units\n",
    "        # self.hidden: 1, batch_size, enc_units\n",
    "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
    "\n",
    "        return output, self.hidden\n",
    "\n",
    "    def initialize_hidden_state(self, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:45.310712200Z",
     "start_time": "2023-10-26T12:24:45.307112600Z"
    }
   },
   "id": "1e383c19d875b9da"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, dec_units: int, enc_units: int, batch_sz: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim + self.enc_units,\n",
    "                          self.dec_units,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.V = nn.Linear(self.enc_units, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor, enc_output: torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
    "        enc_output = enc_output.permute(1,0,2)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden size) we convert it to (batch_size, 1, hidden size)\n",
    "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
    "\n",
    "        # score: (batch_size, max_length, hidden_size) # Bahdanaus's\n",
    "        # It doesn't matter which FC we pick for each of the inputs\n",
    "        score = self.V(torch.tanh(self.W2(enc_output) + self.W1(hidden_with_time_axis)))\n",
    "\n",
    "        #attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = nn.Softmax(dim=1)(score)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = torch.sum(attention_weights * enc_output, dim=1)\n",
    "\n",
    "        # pass the context vector into embedding layer\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # concatenate the context vector and x\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output =  output.view(-1, output.size(2))\n",
    "\n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def initialize_hidden_state(self) -> torch.Tensor:\n",
    "        return torch.zeros((1, self.batch_sz, self.dec_units))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:45.317134100Z",
     "start_time": "2023-10-26T12:24:45.312705800Z"
    }
   },
   "id": "ea8d90b53213b3fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set hyper parameters of the model and load checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee80f61d2b198a1a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Decoder(\n  (embedding): Embedding(61679, 256)\n  (gru): GRU(768, 512, batch_first=True)\n  (fc): Linear(in_features=512, out_features=61679, bias=True)\n  (W1): Linear(in_features=512, out_features=512, bias=True)\n  (W2): Linear(in_features=512, out_features=512, bias=True)\n  (V): Linear(in_features=512, out_features=1, bias=True)\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_OUT = \"../models/gru-detoxification/\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "\n",
    "encoder.load_state_dict(torch.load(PATH_OUT + \"encoder-final.pt\"))\n",
    "decoder.load_state_dict(torch.load(PATH_OUT + \"decoder-final.pt\"))\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:50.881333600Z",
     "start_time": "2023-10-26T12:24:50.440315700Z"
    }
   },
   "id": "3da6e5f7a752cf4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7028f8431c11f9fe"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def pad_sequences(x: list, max_len: int) -> list:\n",
    "    \"\"\"\n",
    "    add padding\n",
    "    :param x: token's sentence\n",
    "    :param max_len: max length of words from column\n",
    "    :return: token's sentence with padding\n",
    "    \"\"\"\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len: padded[:] = x[:max_len]\n",
    "    else: padded[:len(x)] = x\n",
    "    return padded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:24:53.701771100Z",
     "start_time": "2023-10-26T12:24:53.694791400Z"
    }
   },
   "id": "979f0b1ada5e2bf2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['i', 'm', 'tired', '.']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(encoder: Encoder, decoder: Decoder, sentence: str, special_tokens: bool=True) -> list:\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    sentence = torch.unsqueeze(sentence, dim=1)\n",
    "    with torch.no_grad():\n",
    "        enc_output, enc_hidden = encoder(sentence.to(device), device)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)\n",
    "\n",
    "        out_sentence = []\n",
    "        for t in range(1, sentence.size(0)):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device),\n",
    "                                                 dec_hidden.to(device),\n",
    "                                                 enc_output.to(device))\n",
    "            dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
    "            out_word = targ_lang.idx2word[predictions.squeeze().argmax().item()]\n",
    "            if special_tokens:\n",
    "                out_sentence.append(out_word)\n",
    "            else:\n",
    "                if out_word != \"<pad>\" and out_word != \"<end>\":\n",
    "                    out_sentence.append(out_word)\n",
    "                    \n",
    "            \n",
    "    return out_sentence\n",
    "\n",
    "encoder.batch_sz = 1\n",
    "encoder.initialize_hidden_state(device)\n",
    "decoder.batch_sz = 1\n",
    "decoder.initialize_hidden_state()\n",
    "\n",
    "test_sentence = \"<start> i am fucking tired . <end>\"\n",
    "test_sentence = [inp_lang.word2idx[s] for s in test_sentence.split(' ')]\n",
    "test_sentence = pad_sequences(test_sentence, max_length_inp)\n",
    "ret = inference(encoder, decoder, torch.tensor(test_sentence), special_tokens=False)\n",
    "ret"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T12:28:47.044736600Z",
     "start_time": "2023-10-26T12:28:46.440735800Z"
    }
   },
   "id": "689e6946ded881a6"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T11:27:52.697561500Z",
     "start_time": "2023-10-26T11:27:52.693070400Z"
    }
   },
   "id": "8efd59da3719688"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
