{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:46.794813300Z",
     "start_time": "2023-11-03T03:41:46.761215400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "PATH = \"../data/inheritim/\"\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01160619b1f4a9",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH + 'jigsaw.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.820107400Z",
     "start_time": "2023-11-03T03:41:46.767699300Z"
    }
   },
   "id": "ef30eec5f806fc9b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n...                  ...                                                ...   \n159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n\n        toxic  non_toxic  length  \n0           0          1      43  \n1           0          1      17  \n2           0          1      42  \n3           0          1     113  \n4           0          1      13  \n...       ...        ...     ...  \n159566      0          1      47  \n159567      0          1      18  \n159568      0          1      12  \n159569      0          1      25  \n159570      0          1      36  \n\n[159571 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>non_toxic</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>159566</th>\n      <td>ffe987279560d7ff</td>\n      <td>\":::::And for the second time of asking, when ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>159567</th>\n      <td>ffea4adeee384e90</td>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>159568</th>\n      <td>ffee36eab5c267c9</td>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>159569</th>\n      <td>fff125370e4aaaf3</td>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>159570</th>\n      <td>fff46fc426af1f9a</td>\n      <td>\"\\nAnd ... I really don't think you understand...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>36</td>\n    </tr>\n  </tbody>\n</table>\n<p>159571 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.833974100Z",
     "start_time": "2023-11-03T03:41:47.821104600Z"
    }
   },
   "id": "60d9a06783b701a7"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77986e9f02bac17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.914570100Z",
     "start_time": "2023-11-03T03:41:47.828991600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_valid = train_test_split(train_df, test_size=0.2, stratify=train_df[\"toxic\"], random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95367ee20447b13e",
   "metadata": {},
   "source": [
    "let's take equal number of toxic and non-toxic comments since our data imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e930efd7f2ecc791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.916562600Z",
     "start_time": "2023-11-03T03:41:47.875626300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat(\n",
    "    [\n",
    "        df_train[df_train[\"toxic\"] == 0].sample(\n",
    "            10000, random_state=199, replace=False\n",
    "        ),\n",
    "        df_train[df_train[\"toxic\"] == 1].sample(\n",
    "            10000, random_state=199, replace=False\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "df_valid = pd.concat(\n",
    "    [\n",
    "        df_valid[df_valid[\"toxic\"] == 0].sample(\n",
    "            1000, random_state=199, replace=False\n",
    "        ),\n",
    "        df_valid[df_valid[\"toxic\"] == 1].sample(\n",
    "            1000, random_state=199, replace=False\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94deb1c7832f7c8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.917560200Z",
     "start_time": "2023-11-03T03:41:47.901812500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((20000, 5), (2000, 5))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd138547a9eae02e",
   "metadata": {},
   "source": [
    "## Custom Dataset and load it to Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a4615482c142c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.917560200Z",
     "start_time": "2023-11-03T03:41:47.907586100Z"
    }
   },
   "outputs": [],
   "source": [
    "def clear_str(string: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for text\n",
    "    \"\"\"\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    string = re.sub(r\"([?.!,¿])\", r\" \\1 \", string)\n",
    "    string = re.sub(r'[\" \"]+', \" \", string)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    string = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "class ToxicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.df = dataframe\n",
    "\n",
    "    def row_to_tensor(self, tokenizer: BertTokenizer, row: pd.Series, max_len: int=120):\n",
    "        tokens = tokenizer.encode(clear_str(row[\"comment_text\"]), add_special_tokens=True, max_length=max_len, truncation=True)\n",
    "        x = torch.LongTensor(tokens)\n",
    "\n",
    "        labels = ['non_toxic', 'toxic']\n",
    "        y = torch.FloatTensor(row[labels])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.row_to_tensor(self.tokenizer, self.df.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c5dd72f39057e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:47.917560200Z",
     "start_time": "2023-11-03T03:41:47.910806400Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch: torch.Tensor, device: torch.device) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    merges a list of samples to form a mini-batch of Tensors\n",
    "    :param batch: batch of data\n",
    "    :param device: cpu or gpu\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "collate_fn = partial(collate_fn, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4f42aa118f60e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:48.121692600Z",
     "start_time": "2023-11-03T03:41:47.914570100Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "train_dataset = ToxicDataset(tokenizer, df_train)\n",
    "val_dataset = ToxicDataset(tokenizer, df_valid)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(val_dataset)\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                            sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                          sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b72ffd656358b",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fa53db3901b6b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:48.121692600Z",
     "start_time": "2023-11-03T03:41:48.120012100Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert: BertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_x = x[1] \n",
    "        cls_x = self.classifier(cls_x)\n",
    "        out = self.softmax(cls_x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8bf2db5a39ad89",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5691e46dcfc08397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:48.136961700Z",
     "start_time": "2023-11-03T03:41:48.124683300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(model, iterator, optimizer, scheduler, loss_func):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    iteration = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        iteration += 1\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "        outputs = model(x, attention_mask=mask)\n",
    "        loss = loss_func(outputs, y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\\n\")\n",
    "\n",
    "def evaluate(model, iterator, loss_func, verbose=True):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in iterator:\n",
    "            mask = (x != 0).float()\n",
    "            outputs = model(x, attention_mask=mask)\n",
    "            loss = loss_func(outputs, y)\n",
    "            total_loss += loss\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            y = torch.argmax(y, dim=1)\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    labels = ['toxic']\n",
    "    if verbose:\n",
    "        print(\"\\nROC_AUC for labels:\")\n",
    "        for i, name in enumerate(labels):\n",
    "            print(f\" * {name} - {roc_auc_score(true, pred)}\")\n",
    "        print(f\"\\nEVALUATE LOSS -  {total_loss / len(iterator)}\\n\")\n",
    "\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b30a20d10bad8630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:41:49.420352200Z",
     "start_time": "2023-11-03T03:41:48.129985400Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "EPOCH_NUM = 1\n",
    "lr = 2e-5\n",
    "\n",
    "bert      = BertModel.from_pretrained(bert_model_name)\n",
    "model     = BertClassifier(bert, 2).to(device)\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# The triangle learning rate advances linearly until half of the first epoch, then linearly decays.\n",
    "w_steps = 10 ** 3\n",
    "t_steps = len(train_iterator) * EPOCH_NUM - w_steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, w_steps, t_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54541546c4ff442b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:45:56.875677500Z",
     "start_time": "2023-11-03T03:41:49.421347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== EPOCH 1 ==================================================\n",
      "\n",
      "TRAINING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [03:58<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3446442441403866\n",
      "\n",
      "\n",
      "EVALUATING\n",
      "\n",
      "ROC_AUC for labels:\n",
      " * toxic - 0.9089999999999999\n",
      "\n",
      "EVALUATE LOSS -  0.20584282279014587\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i + 1}\", '=' * 50)\n",
    "    print(\"\\nTRAINING\\n\")\n",
    "    train(model, train_iterator, optimizer, scheduler, loss_func)\n",
    "    print(\"\\nEVALUATING\\n\")\n",
    "    evaluate(model, val_iterator, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4bd6cd9dface1",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40b425abe3bc92f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:45:57.433049900Z",
     "start_time": "2023-11-03T03:45:56.874681200Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_OUT = \"../models/bert-detoxification/\"\n",
    "\n",
    "torch.save(model.state_dict(), PATH_OUT + \"model-final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29aca94713bc89ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T03:45:57.436008300Z",
     "start_time": "2023-11-03T03:45:57.434508700Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
